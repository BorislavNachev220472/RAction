{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbaad434-3bbd-4861-8ce7-68593d5151fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce6f3b4-6e5c-419f-ace4-1dd67de9712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw=/workspace/2024-25ab-fai3-specialisation-project-team-specifix-1/data/segmentation/nnUNet_raw\n",
      "nnUNet_preprocessed=/workspace/2024-25ab-fai3-specialisation-project-team-specifix-1/data/segmentation/nnUNet_preprocessed\n",
      "nnUNet_results=/workspace/2024-25ab-fai3-specialisation-project-team-specifix-1/data/segmentation/nnUNet_results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "'nnUNet_raw':'/workspace/2024-25ab-fai3-specialisation-project-team-specifix-1/data/segmentation/nnUNet_raw',\n",
    "'nnUNet_preprocessed':'/workspace/2024-25ab-fai3-specialisation-project-team-specifix-1/data/segmentation/nnUNet_preprocessed',\n",
    "'nnUNet_results':'/workspace/2024-25ab-fai3-specialisation-project-team-specifix-1/data/segmentation/nnUNet_results'\n",
    "}\n",
    "\n",
    "with open('.env', 'w') as file:\n",
    "    for key in data:\n",
    "        file.write(f'{key}={data[key]}\\n')\n",
    "\n",
    "with open('.env', 'r') as file:\n",
    "    cf = file.read()\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "300e1e5f-48a4-41c8-bcd1-25e6aec6a7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    " \n",
    "   # Load environment variables from .env file\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b99e0e8-e34e-4fc8-b63f-d1a6201b8196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU is CUDA:3\n",
      "CUDA:0 NVIDIA RTX A6000, 48669.75MB\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "print(f\"Using GPU is CUDA:{os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    info = torch.cuda.get_device_properties(i)\n",
    "    print(f\"CUDA:{i} {info.name}, {info.total_memory / 1024 ** 2}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4982a0-7e40-4267-a64e-e62a054cb028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "/root/.cache/pypoetry/virtualenvs/specifix-pAjDTqDi-py3.9/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "################### Loading pretrained weights from file  /workspace/2024-25ab-fai3-specialisation-project-team-specifix-1/data/segmentation/nnUNet_results/Dataset019_ulna/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/checkpoint_final.pth ###################\n",
      "Below is the list of overlapping blocks in pretrained model and nnUNet architecture:\n",
      "encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])\n",
      "encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])\n",
      "encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])\n",
      "encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])\n",
      "encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])\n",
      "encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])\n",
      "encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])\n",
      "encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])\n",
      "encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])\n",
      "encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])\n",
      "encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])\n",
      "encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])\n",
      "encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])\n",
      "encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])\n",
      "encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])\n",
      "decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])\n",
      "decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])\n",
      "decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])\n",
      "decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])\n",
      "decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])\n",
      "decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])\n",
      "decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])\n",
      "decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])\n",
      "decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])\n",
      "decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])\n",
      "decoder.stages.0.convs.0.conv.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.norm.weight shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.norm.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])\n",
      "decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.stages.0.convs.1.conv.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.norm.weight shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.norm.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])\n",
      "decoder.stages.1.convs.0.conv.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.norm.weight shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.norm.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])\n",
      "decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "decoder.stages.1.convs.1.conv.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.norm.weight shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.norm.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])\n",
      "decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])\n",
      "decoder.stages.2.convs.0.conv.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.norm.weight shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.norm.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])\n",
      "decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "decoder.stages.2.convs.1.conv.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.norm.weight shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.norm.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])\n",
      "decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])\n",
      "decoder.stages.3.convs.0.conv.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.norm.weight shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.norm.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])\n",
      "decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "decoder.stages.3.convs.1.conv.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.norm.weight shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.norm.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])\n",
      "decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])\n",
      "decoder.stages.4.convs.0.conv.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.norm.weight shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.norm.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])\n",
      "decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "decoder.stages.4.convs.1.conv.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.norm.weight shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.norm.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])\n",
      "decoder.transpconvs.0.weight shape torch.Size([320, 320, 2, 1, 2])\n",
      "decoder.transpconvs.0.bias shape torch.Size([320])\n",
      "decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])\n",
      "decoder.transpconvs.1.bias shape torch.Size([256])\n",
      "decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])\n",
      "decoder.transpconvs.2.bias shape torch.Size([128])\n",
      "decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])\n",
      "decoder.transpconvs.3.bias shape torch.Size([64])\n",
      "decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])\n",
      "decoder.transpconvs.4.bias shape torch.Size([32])\n",
      "################### Done ###################\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 112, 160], 'median_image_size_in_voxels': [512.0, 512.0, 706.0], 'spacing': [0.5999999642372131, 0.3466796278953552, 0.3466796278953552], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [5, 4, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 1, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset050_new_hierarchy_f', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [0.5999999642372131, 0.3466796278953552, 0.3466796278953552], 'original_median_shape_after_transp': [512, 512, 616], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 4087.0, 'mean': 2174.0908203125, 'median': 2179.0, 'min': 1117.0, 'percentile_00_5': 1254.0, 'percentile_99_5': 3336.0, 'std': 633.0347290039062}}} \n",
      "\n",
      "2024-11-27 09:07:40.524011: unpacking dataset...\n",
      "2024-11-27 09:07:44.749326: unpacking done...\n",
      "2024-11-27 09:07:44.750193: do_dummy_2d_data_aug: False\n",
      "2024-11-27 09:07:44.750946: Using splits from existing split file: /workspace/2024-25ab-fai3-specialisation-project-team-specifix-1/data/segmentation/nnUNet_preprocessed/Dataset050_new_hierarchy_f/splits_final.json\n",
      "2024-11-27 09:07:44.751159: The split file contains 5 splits.\n",
      "2024-11-27 09:07:44.751229: Desired fold for training: 0\n",
      "2024-11-27 09:07:44.751297: This split has 48 training and 12 validation cases.\n",
      "2024-11-27 09:07:44.803833: Unable to plot network architecture:\n",
      "2024-11-27 09:07:44.803960: No module named 'hiddenlayer'\n",
      "2024-11-27 09:07:44.860612: \n",
      "2024-11-27 09:07:44.860742: Epoch 0\n",
      "2024-11-27 09:07:44.860964: Current learning rate: 0.01\n",
      "using pin_memory on device 0\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "!poetry run nnUNetv2_train Dataset050_new_hierarchy_f 3d_fullres 0 -pretrained_weights \"/workspace/2024-25ab-fai3-specialisation-project-team-specifix-1/data/segmentation/nnUNet_results/Dataset019_ulna/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/checkpoint_final.pth\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c690478c-6b2e-4237-8419-f64440fa21f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continue Training:\n",
    "nnUNetv2_train Dataset019_ulna 3d_fullres 0 --c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f8bbe-4d0f-43fd-a288-5d5a9595f58c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Poetry)",
   "language": "python",
   "name": "poetry-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
